# Generation Timeout Fix - 2025-11-16

**Status:** ‚úÖ FIXED and DEPLOYED

## Problem

Training was getting stuck at step ~2500 for hours due to `model.generate()` hanging indefinitely during evaluation. The process would:
- Freeze at step 2500 for 2.5+ hours
- Show 60% GPU utilization but make no progress
- Never timeout or recover

## Root Cause

1. **No timeout protection:** `model.generate()` could run forever
2. **Pathological examples:** Some training examples caused infinite/very slow generation
3. **Config had timeout setting but it wasn't implemented:** `eval_timeout_seconds: 60` in config.json was unused

## Solution Implemented

Added **signal-based timeout protection** using Unix signals (SIGALRM):

### Code Changes (train.py)

**1. Import signal module:**
```python
import signal  # Line 26
```

**2. Timeout exception and handler:**
```python
# Line 562-567
class GenerationTimeout(Exception):
    pass

def timeout_handler(signum, frame):
    raise GenerationTimeout("Model generation timed out")
```

**3. Wrap model.generate() with timeout:**
```python
# Lines 688-717
# Set timeout (60 seconds)
signal.signal(signal.SIGALRM, timeout_handler)
signal.alarm(60)

try:
    outputs = self.model_ref.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.1,
        do_sample=False,
        pad_token_id=self.tokenizer.eos_token_id,
        eos_token_id=self.tokenizer.eos_token_id
    )
    signal.alarm(0)  # Cancel alarm on success

    model_output = self.tokenizer.decode(...)

except GenerationTimeout:
    signal.alarm(0)  # Cancel alarm
    print(f"\n‚ö†Ô∏è  TIMEOUT: Model generation exceeded 60s at step {state.global_step}")
    print(f"   Skipping model output for this example")
    model_output = None
except Exception as e:
    signal.alarm(0)  # Cancel alarm
    print(f"\n‚ö†Ô∏è  Generation error at step {state.global_step}: {e}")
    model_output = None
```

## How It Works

**Before timeout:**
1. Set signal handler to raise `GenerationTimeout` on SIGALRM
2. Set 60-second alarm
3. Call `model.generate()`

**Normal completion:**
1. Generate completes in <60 seconds
2. Cancel alarm with `signal.alarm(0)`
3. Decode and return output

**Timeout scenario:**
1. Generate runs >60 seconds
2. SIGALRM fires ‚Üí timeout_handler raises exception
3. Exception caught ‚Üí cancel alarm
4. Set `model_output = None`
5. Training continues to next step

## Recovery Behavior

**When timeout occurs:**
- ‚úÖ Generation interrupted immediately
- ‚úÖ Alarm cancelled (cleanup)
- ‚úÖ `model_output = None` (already handled downstream)
- ‚úÖ Loss calculation continues (doesn't need model output)
- ‚úÖ Status JSON updates normally
- ‚úÖ Training proceeds to next step

**No data loss or corruption:**
- Training state preserved
- Checkpoints unaffected
- Just missing model output for that one eval step

## Why This Won't Repeat

**Multiple layers of protection:**

1. **Timeout protection:** 60s hard limit prevents infinite hangs
2. **New config:** `eval_steps=200` (was 10) = 20x fewer evals
3. **Graceful degradation:** Missing outputs don't break training
4. **Different examples:** Each eval uses different training data

**Worst case:**
- Occasional timeout on pathological examples (~1-5%)
- You see: "‚ö†Ô∏è TIMEOUT: Model generation exceeded 60s at step X"
- Training continues normally
- Eventually completes successfully

**Expected behavior:**
- Most evals complete in 2-10 seconds
- Rare timeouts on difficult examples
- Clear warnings in logs
- No freezing or hanging

## What You'll See

**Normal eval (step 200, 400, 600...):**
```
üîç FULL EVAL - Step 200
üìù PROMPT: [example]
‚úÖ GOLDEN (57 tokens): [expected answer]
ü§ñ MODEL (61 tokens): [generated answer]
‚úÖ MATCH
üìâ LOSS ON THIS EXAMPLE: 0.7942
üìä Loss Comparison: Train=0.7942 | Val=0.8123 | Gap=+0.0181 ‚úÖ
```

**Timeout (rare):**
```
üîç FULL EVAL - Step 2500
üìù PROMPT: [example]
‚úÖ GOLDEN (57 tokens): [expected answer]

‚ö†Ô∏è  TIMEOUT: Model generation exceeded 60s at step 2500
   Skipping model output for this example

ü§ñ MODEL: [Skipped - fast log only]
üìâ LOSS ON THIS EXAMPLE: 0.7942
```

## Testing

**Syntax check:** ‚úÖ Passed
```bash
python3 -m py_compile train.py
# No errors
```

**Deployment:**
- Killed stuck process (PID 236783)
- Restarted daemon with fixed code (PID 245689)
- Training resumed from checkpoint-2488
- Will continue to completion

## Monitoring

Watch for timeout warnings:
```bash
tail -f training_output.log | grep -i timeout
```

Check if training progressing:
```bash
cat status/training_status.json | jq '{step: .current_step, loss: .loss}'
```

## Additional Benefits

The new two-tier logging system also helps:
- **Fast logs every 10 steps:** No model generation, super fast
- **Full eval every 200 steps:** Model generation only when needed
- **20x fewer opportunities to hit timeout**

## Files Modified

- `train.py` - Added signal import, timeout handler, wrapped generate() call

## Next Steps

Monitor training to verify:
1. No more freezes at step 2500
2. Training progresses smoothly
3. Occasional timeout messages are handled gracefully
4. Training completes successfully

---

**If you see timeout warnings frequently (>10%), that indicates a model or data issue that should be investigated. But training will still complete!**
