# =============================================================================
# MEMORY PROFILE: 24GB LoRA (Full-Precision Base)
# =============================================================================
# LoRA adapters on full-precision base model (no quantization)
# Better accuracy than QLoRA, but requires more VRAM
# Best for: 4B-7B models where quantization hurts performance
# =============================================================================

id: 24gb_lora
name: "24GB LoRA Profile"
rpg_name: "The Precise Blade"
icon: "üó°Ô∏è"
description: |
  LoRA adapters on full-precision base model.

  Trade-offs vs QLoRA:
  + Better accuracy (no quantization artifacts)
  + Faster inference (no dequantization overhead)
  - Higher VRAM usage (~2x vs QLoRA for same model)

  Use when: Quantization degrades model quality unacceptably.

target_vram_gb: 24

# =============================================================================
# PEFT CONFIGURATION
# =============================================================================
peft:
  method: "lora"  # NOT qlora - base is full precision

  lora:
    r: 64                    # Rank (higher = more capacity)
    alpha: 128               # Scaling (typically 2x rank)
    dropout: 0.05            # Regularization
    target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
    bias: "none"
    task_type: "CAUSAL_LM"

  galore:
    rank: 1024
    update_proj_gap: 200
    scale: 0.25
    proj_type: "std"

# =============================================================================
# QUANTIZATION - DISABLED
# Base model stays in full precision (bf16/fp16)
# =============================================================================
quantization:
  load_in_4bit: false
  load_in_8bit: false
  bnb_4bit_compute_dtype: "bfloat16"
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_use_double_quant: false

# =============================================================================
# MEMORY-EFFICIENT IMPLEMENTATIONS
# =============================================================================
memory:
  gradient_checkpointing: true   # Essential - recompute activations
  flash_attention: true          # O(N) attention memory
  paged_optimizer: false         # Not needed for smaller models
  cpu_offload: false

# =============================================================================
# OPTIMIZER CONFIGURATION
# 8-bit optimizer to save memory on optimizer states
# =============================================================================
optimizer:
  type: "adamw_8bit"  # 50% less optimizer memory vs 32-bit

  adamw:
    lr: 2e-4                 # LoRA can use higher LR than full training
    betas: [0.9, 0.999]
    eps: 1e-8
    weight_decay: 0.01

  muon:
    hidden_lr: 0.02
    aux_lr: 3e-4
    momentum: 0.95
    weight_decay: 0

# =============================================================================
# TRAINING CONSTRAINTS
# =============================================================================
constraints:
  max_batch_size: 1
  recommended_grad_accum: 8
  max_sequence_length: 2048  # Can go higher than QLoRA
  precision: "bf16"

# =============================================================================
# MODEL SIZE RECOMMENDATIONS
# =============================================================================
recommendations:
  max_model_size_b: 7
  optimal_model_sizes:
    - "4B"
    - "7B"
  tested_models:
    - "Qwen3-4B"
    - "Mistral-7B"
    - "Llama-3-7B"
  notes: |
    Full-precision LoRA for 4B-7B models.

    Expected VRAM usage (7B model):
    - Model (bf16): ~14GB
    - LoRA adapters: ~0.5GB
    - Activations: ~6GB (with checkpointing)
    - Optimizer (8-bit): ~2GB
    - Total: ~22-23GB peak

    For 8B+ models, use 24gb_qlora instead.
