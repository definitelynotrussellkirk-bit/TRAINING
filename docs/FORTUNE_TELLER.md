# THE FORTUNE TELLER

**Surprise-Weighted Training for Efficient Learning**

---

## ğŸ”® Overview

THE FORTUNE TELLER is a training enhancement that focuses gradient updates on tokens that **surprise** the model, rather than treating all tokens equally.

**Core Idea**: Tokens the model predicts confidently and correctly shouldn't receive the same gradient weight as uncertain predictions. By focusing on surprises, training becomes more efficient and naturally implements a form of curriculum learning.

---

## ğŸ¯ The Problem

Standard cross-entropy loss treats all tokens equally:
- Model predicts "the" correctly with 99% confidence â†’ Full gradient update
- Model struggles between "their" vs "there" with 50/50 confidence â†’ Same gradient update

This is wasteful. The model has already mastered "the" - why spend gradient budget on it?

---

## ğŸ’¡ The Solution

**Surprise-weighted loss**: Modulate gradient contributions by how "surprised" the model is:

```
weighted_loss = cross_entropy_loss Ã— surprise_weight
```

Where `surprise_weight` is computed per-token based on:
1. **Entropy**: How uncertain is the distribution?
2. **Confidence**: How low is the max probability?
3. **Perplexity**: How unexpected is the correct token?
4. **Margin**: How close is the second-best option?

---

## ğŸ“Š Surprise Metrics

### 1. Entropy (Default)

```python
H = -Î£ p(x) log p(x)
```

- **High entropy** = Model is uncertain = High surprise
- **Low entropy** = Model is confident = Low surprise
- **Range**: [0, log(vocab_size)]
- **Best for**: General-purpose surprise detection

### 2. Confidence

```python
surprise = 1 - max(p)
```

- **Low max prob** = Not confident = High surprise
- **High max prob** = Confident = Low surprise
- **Range**: [0, 1]
- **Best for**: Simple, interpretable weighting

### 3. Perplexity

```python
perplexity = exp(-log p(correct_token))
```

- **High perplexity** = Correct token was unexpected = High surprise
- **Low perplexity** = Correct token was expected = Low surprise
- **Range**: [1, âˆ]
- **Best for**: Emphasizing truly unexpected correct tokens

### 4. Margin

```python
margin = p(correct) - p(second_best)
surprise = 1 - margin
```

- **Small margin** = Close competition = High surprise
- **Large margin** = Clear winner = Low surprise
- **Range**: [0, 1]
- **Best for**: Multi-way classification uncertainty

---

## ğŸ® RPG Integration

**In-game lore**: The Fortune Teller is an oracle who predicts what will challenge DIO most, guiding training effort to where it's needed.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”® THE FORTUNE TELLER              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  "I see... uncertainty ahead."      â”‚
â”‚                                     â”‚
â”‚  Current Reading:                   â”‚
â”‚  â€¢ Avg Surprise:    2.34            â”‚
â”‚  â€¢ Surprise Std:    1.87            â”‚
â”‚  â€¢ Training Focus:  HIGH            â”‚
â”‚                                     â”‚
â”‚  Prediction: DIO will struggle      â”‚
â”‚  with syllogistic reasoning at      â”‚
â”‚  Level 15. Recommend extra XP.      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš™ï¸ Configuration

### Enable in `config.json`

```json
{
  "fortune_teller": {
    "enabled": true,
    "surprise_metric": "entropy",
    "min_surprise": 0.1,
    "max_surprise": 10.0,
    "normalize_batch": true,
    "temperature": 1.0,
    "save_history": true,
    "history_path": null
  }
}
```

### Parameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| `enabled` | `false` | Enable Fortune Teller loss |
| `surprise_metric` | `"entropy"` | Which metric to use (entropy, confidence, perplexity, margin) |
| `min_surprise` | `0.1` | Minimum weight (prevents vanishing gradients) |
| `max_surprise` | `10.0` | Maximum weight (prevents explosion) |
| `normalize_batch` | `true` | Normalize surprises within each batch |
| `temperature` | `1.0` | Temperature for scaling (higher = more uniform weights) |
| `save_history` | `true` | Save surprise metrics to JSON |
| `history_path` | `null` | Path to save history (default: output_dir/fortune_teller_history.json) |

---

## ğŸ§ª Usage Examples

### 1. Basic Usage

```python
from trainer.losses import FortuneTellerLoss

loss_fn = FortuneTellerLoss(
    surprise_metric="entropy",
    min_surprise=0.1,
    normalize_batch=True,
)

# During training
loss, details = loss_fn(logits, labels, return_details=True)
print(f"Loss: {loss.item():.4f}")
print(f"Avg surprise: {details['avg_surprise']:.4f}")
```

### 2. With TrainerEngine

```python
from trainer.core import TrainerEngine
from trainer.config import TrainerConfig

# Create config with Fortune Teller enabled
config = TrainerConfig(...)
config.fortune_teller.enabled = True
config.fortune_teller.surprise_metric = "entropy"

# Run training
engine = TrainerEngine()
result = engine.run_job(config)

# History saved automatically to output_dir/fortune_teller_history.json
```

### 3. Tracking and Analysis

```python
from trainer.losses import FortuneTellerTracker

tracker = FortuneTellerTracker()

# During training
_, details = loss_fn(logits, labels, return_details=True)
tracker.update(step, details)

# Get statistics
stats = tracker.get_stats(window=100)
print(f"Recent avg surprise: {stats['avg_surprise']:.3f}")

# Save to disk
tracker.save("fortune_teller_history.json")
```

### 4. Visualization

```bash
# Test metrics on synthetic data
python3 scripts/test_fortune_teller.py --test-metrics

# Visualize training history
python3 scripts/test_fortune_teller.py --visualize results/fortune_teller_history.json
```

---

## ğŸ¯ Expected Behavior

### During Training

**Early stages** (high surprise everywhere):
- All tokens are surprising
- Weights are relatively uniform
- Falls back to near-standard training
- **This is expected and good!**

**Mid stages** (differentiation):
- Easy patterns (grammar, common words) â†’ Low surprise â†’ Low weight
- Hard patterns (reasoning, rare words) â†’ High surprise â†’ High weight
- **Automatic curriculum emerges!**

**Late stages** (mastery):
- Most tokens have low surprise
- Gradient focus on remaining difficult cases
- **Efficient fine-tuning**

### Surprise Evolution

Typical surprise curve over training:

```
Surprise
   ^
   â”‚     â•±â€¾â€¾â€¾â•²
 3 â”‚    â•±      â•²___
   â”‚   â•±           â•²___
 2 â”‚  â•±                â•²___
   â”‚ â•±                     â•²___
 1 â”‚â•±                           â•²___
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€> Steps
   0    5k    10k   15k   20k   25k
```

1. **Initial rise**: Model learns basic patterns, surprise increases as it becomes aware of what it doesn't know
2. **Plateau**: Steady learning on core curriculum
3. **Decline**: As patterns are mastered, surprise decreases
4. **Long tail**: Remaining surprise focused on hardest cases

---

## âš ï¸ Edge Cases & Mitigations

### 1. Overconfident Wrong Predictions

**Problem**: Model confidently predicts wrong token â†’ Low surprise but high loss

**Mitigation**: Surprise is multiplied by loss, so high loss still gets gradient signal

**Math**:
```python
weighted_loss = cross_entropy_loss Ã— surprise_weight
```
If CE loss is high (wrong prediction), weighted loss is still high regardless of surprise.

### 2. Vanishing Gradients (All Low Surprise)

**Problem**: Everything becomes "easy", surprise drops to zero, no learning

**Mitigation**: `min_surprise` parameter ensures minimum weight

**Config**:
```json
{"min_surprise": 0.1}  // At least 10% of standard gradient
```

### 3. Exploding Gradients (All High Surprise)

**Problem**: Everything is surprising, weights blow up

**Mitigation**: `max_surprise` clipping and batch normalization

**Config**:
```json
{
  "max_surprise": 10.0,
  "normalize_batch": true
}
```

### 4. Batch-Level Variance

**Problem**: Some batches have all easy examples, some all hard â†’ inconsistent gradients

**Mitigation**: Batch normalization standardizes surprise distribution per batch

**Effect**: Each batch gets a balanced distribution of weights, regardless of absolute difficulty

### 5. Temperature Scaling

**Problem**: Need to control how sharply surprise affects weights

**Solution**: Temperature parameter smooths or sharpens the distribution

```python
surprise = surprise / temperature

# temperature = 0.5 â†’ Sharper focus on high-surprise tokens
# temperature = 1.0 â†’ Standard scaling (default)
# temperature = 2.0 â†’ More uniform, gentler focusing
```

---

## ğŸ“ˆ Predicted Effects

### Positive

1. **Efficient Learning**: Don't waste gradients on mastered patterns
2. **Automatic Curriculum**: Naturally focuses on progressively harder content
3. **Reduced Forgetting**: Confident correct predictions have low gradient â†’ less likely to be unlearned
4. **Better Generalization**: Focus on uncertain/novel patterns improves robustness
5. **Faster Convergence**: Gradient budget spent where it matters

### Potential Issues

1. **Confidence Calibration**: If model becomes overconfident incorrectly, surprise drops but performance doesn't improve
2. **Metric Sensitivity**: Different surprise metrics may behave differently for your data
3. **Hyperparameter Tuning**: min/max surprise, temperature need tuning
4. **Computational Cost**: Extra forward pass operations (entropy, softmax, etc.) per token

---

## ğŸ§ª Experiments & Validation

### Recommended Experiments

1. **Baseline Comparison**:
   - Train same model with/without Fortune Teller
   - Compare: final loss, convergence speed, eval metrics

2. **Metric Ablation**:
   - Try all 4 surprise metrics (entropy, confidence, perplexity, margin)
   - Find which works best for your domain

3. **Hyperparameter Sweep**:
   - `min_surprise`: [0.01, 0.1, 0.3]
   - `temperature`: [0.5, 1.0, 2.0]
   - `normalize_batch`: [true, false]

4. **Curriculum Analysis**:
   - Track which tokens have high surprise over time
   - Verify automatic progression from easy â†’ hard

5. **Confidence Calibration**:
   - Measure model calibration (are probabilities accurate?)
   - Compare standard vs Fortune Teller calibration

### Metrics to Track

- **Training**: Loss, surprise (mean, std, max, min), gradient norms
- **Evaluation**: Accuracy, perplexity, calibration error
- **Efficiency**: Steps to convergence, total FLOPs
- **Curriculum**: Surprise distribution evolution, easy vs hard token accuracy

---

## ğŸ—‚ï¸ File Structure

```
trainer/
â”œâ”€â”€ losses/
â”‚   â”œâ”€â”€ __init__.py                      # Export FortuneTellerLoss
â”‚   â””â”€â”€ fortune_teller.py                # Core implementation
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ engine.py                        # Integration with TrainerEngine
â”‚   â””â”€â”€ fortune_teller_trainer.py        # Custom Trainer class
â””â”€â”€ config/
    â””â”€â”€ schema.py                        # FortuneTellerConfig dataclass

scripts/
â””â”€â”€ test_fortune_teller.py               # Testing and visualization

docs/
â””â”€â”€ FORTUNE_TELLER.md                    # This file
```

---

## ğŸš€ Quick Start

### 1. Test the Implementation

```bash
# Test all surprise metrics
python3 scripts/test_fortune_teller.py --test-metrics
```

### 2. Enable for Training

Edit `config.json`:

```json
{
  "fortune_teller": {
    "enabled": true,
    "surprise_metric": "entropy"
  }
}
```

### 3. Run Training

```bash
USE_ENGINE=1 python3 core/train.py --dataset data/train.jsonl --yes
```

### 4. Analyze Results

```bash
# Visualize surprise over training
python3 scripts/test_fortune_teller.py --visualize models/current_model/fortune_teller_history.json
```

---

## ğŸ“š Theory & Background

### Relation to Curriculum Learning

Fortune Teller implements **automatic curriculum learning**:
- Standard curriculum: Manually design easy â†’ hard progression
- Fortune Teller: Model discovers its own curriculum via surprise

As training progresses:
- Initially mastered patterns become "easy" (low surprise)
- Remaining difficult patterns stay "hard" (high surprise)
- Gradient budget automatically shifts to hard cases

### Relation to Importance Sampling

Similar to importance sampling in RL:
- Sample high-reward experiences more often
- Fortune Teller: Weight high-surprise tokens more heavily

### Relation to Active Learning

Active learning: Query labels for most uncertain examples
Fortune Teller: Weight gradients by uncertainty (surprise)

Both focus compute on informative data points.

---

## ğŸ“ Future Directions

### Token-Level Primitives

Track surprise per primitive pattern:
- High surprise on syllogisms â†’ Need more L15-L20 training
- Low surprise on binary arithmetic â†’ Can reduce BIN training

### Dynamic Curriculum Adjustment

Use surprise to auto-adjust difficulty:
```python
if avg_surprise < 0.5:
    level_up()  # Too easy, increase difficulty
elif avg_surprise > 3.0:
    level_down()  # Too hard, decrease difficulty
```

### Meta-Learning

Learn the surprise metric itself:
- Train a small network to predict optimal weights
- Use surprise history to improve weighting strategy

### Multi-Task Weighting

Different skills have different surprise profiles:
- Weight tasks by their current surprise
- Focus on skills that need work

---

## ğŸ“ Troubleshooting

### Training Loss Doesn't Decrease

**Check**:
- Is `min_surprise` too high? (Try 0.01)
- Is `normalize_batch` enabled? (Try disabling)
- Are all tokens unsurprising? (Check surprise history)

### Gradients Explode

**Check**:
- Is `max_surprise` set? (Try 10.0)
- Is `normalize_batch` enabled? (Should be)
- Are there NaN values in logits?

### Surprise Doesn't Decrease Over Time

**Check**:
- Is the model learning? (Check standard loss)
- Is the metric appropriate? (Try different surprise_metric)
- Is data too diverse/hard? (Expected for complex curricula)

### Fortune Teller Performs Worse Than Standard

**Possible causes**:
1. Hyperparameters not tuned (try different min/max/temperature)
2. Metric not suited to task (try different surprise_metric)
3. Data already well-curated (Fortune Teller helps most with mixed difficulty)
4. Insufficient training time (benefits compound over time)

---

## ğŸ¯ Summary

**THE FORTUNE TELLER** is a training enhancement that:
- âœ… Focuses gradients where needed (high surprise)
- âœ… Reduces wasted updates (low surprise on mastered patterns)
- âœ… Implements automatic curriculum (easy â†’ hard progression)
- âœ… Integrates seamlessly with TrainerEngine
- âœ… Tracks metrics for analysis and visualization

**When to use**:
- Training on mixed-difficulty data
- Want automatic curriculum learning
- Need efficient use of compute budget
- Researching uncertainty-weighted learning

**When NOT to use**:
- Data is already well-curated (single difficulty level)
- Need exact reproduction of baseline (different gradient trajectory)
- Training time is extremely limited (adds small overhead)

---

**Built with PyTorch â€¢ Compatible with HuggingFace Transformers â€¢ MIT Licensed**
